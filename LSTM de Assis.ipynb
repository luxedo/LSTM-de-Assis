{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM com Embeddings (Feat. Machado de Assis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import re\n",
    "from typing import List, Set, Dict, Tuple, Generator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "CAMINHO_MODELO = \"LSTM-de-assis.h5\"\n",
    "CAMINHO_DICIONARIO = \"dicionario.json\"\n",
    "CAMINHO_DICIONARIO_INDICES = \"dicionario_indices.json\"\n",
    "EMBEDDING_UNITS = 128\n",
    "LSTM_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_dicionario(palavras: List[str], minimo_palavras: int) -> (Dict[str, int], Dict[int, str], Dict[str, int]):\n",
    "    dicionario_freq = collections.Counter(palavras)\n",
    "    palavras_no_dicionario = [palavra for palavra, qtd in dicionario_freq.items() if qtd > minimo_palavras]\n",
    "    dicionario = {palavra: indice for (indice, palavra) in enumerate(palavras_no_dicionario)}\n",
    "    dicionario[\"UNK\"] = len(dicionario)\n",
    "    dicionario_indices = dict(zip(dicionario.values(), dicionario.keys()))\n",
    "    return dicionario, dicionario_indices, dicionario_freq\n",
    "\n",
    "def criar_dataset(minimo_palavras_frase=3, minimo_palavras_dicionario = 2):\n",
    "    df_obras = pd.read_csv(\"./obras_machado_de_assis.csv\")\n",
    "    df_obras = df_obras[df_obras[\"categoria\"] != \"tradução\"]\n",
    "\n",
    "\n",
    "    dataset = pd.Series(np.concatenate(df_obras[\"texto\"]\\\n",
    "                .str.replace(\"\\n+\", \" \")\\\n",
    "                .str.replace(\"\\.+\", \".\")\\\n",
    "                .str.replace(\"\\;\", \",\")\\\n",
    "                .str.split(\"([\\.\\?!][\\'\\\"\\u2018\\u2019\\u201c\\u201d\\)\\]]*\\s*(?<!\\w\\.\\w.)(?<![A-Z][a-z][a-z]\\.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)\\s+)\")))\\\n",
    "                .str.strip()\n",
    "    df_dataset = pd.DataFrame(list(zip(dataset, dataset[1:])))\n",
    "    df_dataset = df_dataset[df_dataset[0].str.strip().apply(lambda row: len(row.split())) != 1]\n",
    "    dataset = (df_dataset[0]+df_dataset[1]).reset_index(drop=True)\n",
    "    dataset = dataset\\\n",
    "        .str.replace(\" *\\. *\", \" . \")\\\n",
    "        .str.replace(\" *\\, *\", \" , \")\\\n",
    "        .str.replace(\" *\\! *\", \" ! \")\\\n",
    "        .str.replace(\" *\\? *\", \" ? \")\\\n",
    "        .str.replace(\"[^\\w\\s\\d\\.\\,\\?\\!]\", \"\")\\\n",
    "        .str.strip()\\\n",
    "        .str.lower()\n",
    "    dataset = dataset[dataset.apply(lambda row: len(row.split())) > minimo_palavras_frase].reset_index(drop=True)\n",
    "    palavras = np.concatenate(dataset.apply(lambda row: row.split()))\n",
    "\n",
    "    dicionario, dicionario_indices, dicionario_freq = criar_dicionario(palavras, minimo_palavras_dicionario)\n",
    "    dataset = dataset.apply(lambda row: [dicionario.get(palavra, len(dicionario)-1) for palavra in row.split()])\n",
    "    return dataset, dicionario, dicionario_indices, dicionario_freq\n",
    "\n",
    "def separar_dataset(dataset):\n",
    "    indices_test = np.random.randint(0, tamanho_dataset, tamanho_dataset//100)\n",
    "    test_dataset = dataset[indices_test].reset_index(drop=True)\n",
    "    mask = np.ones(tamanho_dataset, dtype=bool)\n",
    "    mask[indices_test] = False\n",
    "    dataset = dataset[mask].reset_index(drop=True)\n",
    "    return dataset, test_dataset\n",
    "\n",
    "dataset, dicionario, dicionario_indices, dicionario_freq = criar_dataset()\n",
    "tamanho_dicionario = len(dicionario)\n",
    "tamanho_dataset = len(dataset)\n",
    "\n",
    "dataset, test_dataset = separar_dataset(dataset)\n",
    "\n",
    "with open(CAMINHO_DICIONARIO, \"w\") as fp:\n",
    "    json.dump(dicionario, fp)\n",
    "with open(CAMINHO_DICIONARIO_INDICES, \"w\") as fp:\n",
    "    json.dump(dicionario_indices, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM-de-Assis\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (1, None, 128)            3635072   \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (1, None, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (1, None, 128)            131584    \n",
      "_________________________________________________________________\n",
      "densa1 (TimeDistributed)     (None, None, 128)         16512     \n",
      "_________________________________________________________________\n",
      "densa2 (TimeDistributed)     (None, None, 28399)       3663471   \n",
      "=================================================================\n",
      "Total params: 7,578,223\n",
      "Trainable params: 7,578,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def criar_modelo(dim_entrada, dim_embedding, dim_lstm):\n",
    "    model = tf.keras.models.Sequential(name=\"LSTM-de-Assis\")\n",
    "    model.add(tf.keras.layers.Input((None,), name=\"entrada\", batch_size=1))\n",
    "    model.add(tf.keras.layers.Embedding(dim_entrada, dim_embedding, name='embedding'))\n",
    "    model.add(tf.keras.layers.LSTM(dim_lstm, name=\"lstm1\", activation=\"tanh\", return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(dim_lstm, name=\"lstm2\", activation=\"tanh\", return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(dim_lstm, activation=\"tanh\"), name=\"densa1\"))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(dim_entrada, activation=\"softmax\"), name=\"densa2\"))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "modelo = criar_modelo(tamanho_dicionario, EMBEDDING_UNITS, LSTM_UNITS)\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10181/10181 [==============================] - 645s 63ms/step - loss: 6.5426 - accuracy: 0.0916 - val_loss: 6.1982 - val_accuracy: 0.1098\n",
      "Epoch 2/50\n",
      "10181/10181 [==============================] - 744s 73ms/step - loss: 6.0071 - accuracy: 0.1141 - val_loss: 5.9623 - val_accuracy: 0.1198\n",
      "Epoch 3/50\n",
      "10181/10181 [==============================] - 643s 63ms/step - loss: 5.8661 - accuracy: 0.1256 - val_loss: 5.8698 - val_accuracy: 0.1269\n",
      "Epoch 4/50\n",
      "10181/10181 [==============================] - 638s 63ms/step - loss: 5.7849 - accuracy: 0.1336 - val_loss: 5.7779 - val_accuracy: 0.1323\n",
      "Epoch 5/50\n",
      "10181/10181 [==============================] - 642s 63ms/step - loss: 5.7594 - accuracy: 0.1377 - val_loss: 5.7671 - val_accuracy: 0.1386\n",
      "Epoch 6/50\n",
      "10181/10181 [==============================] - 567s 56ms/step - loss: 5.6964 - accuracy: 0.1417 - val_loss: 5.7345 - val_accuracy: 0.1371\n",
      "Epoch 7/50\n",
      "10181/10181 [==============================] - 550s 54ms/step - loss: 5.6604 - accuracy: 0.1470 - val_loss: 5.6906 - val_accuracy: 0.1418\n",
      "Epoch 8/50\n",
      "10181/10181 [==============================] - 550s 54ms/step - loss: 5.6644 - accuracy: 0.1484 - val_loss: 5.6685 - val_accuracy: 0.1534\n",
      "Epoch 9/50\n",
      "10181/10181 [==============================] - 552s 54ms/step - loss: 5.6219 - accuracy: 0.1518 - val_loss: 5.6879 - val_accuracy: 0.1537\n",
      "Epoch 10/50\n",
      "10181/10181 [==============================] - 544s 53ms/step - loss: 5.6012 - accuracy: 0.1526 - val_loss: 5.6516 - val_accuracy: 0.1529\n",
      "Epoch 11/50\n",
      "10181/10181 [==============================] - 544s 53ms/step - loss: 5.5405 - accuracy: 0.1594 - val_loss: 5.6444 - val_accuracy: 0.1541\n",
      "Epoch 12/50\n",
      "10181/10181 [==============================] - 559s 55ms/step - loss: 5.5321 - accuracy: 0.1607 - val_loss: 5.6722 - val_accuracy: 0.1543\n",
      "Epoch 13/50\n",
      "10181/10181 [==============================] - 520s 51ms/step - loss: 5.5454 - accuracy: 0.1624 - val_loss: 5.5955 - val_accuracy: 0.1574\n",
      "Epoch 14/50\n",
      "10181/10181 [==============================] - 522s 51ms/step - loss: 5.5337 - accuracy: 0.1632 - val_loss: 5.6647 - val_accuracy: 0.1577\n",
      "Epoch 15/50\n",
      "10181/10181 [==============================] - 521s 51ms/step - loss: 5.5232 - accuracy: 0.1630 - val_loss: 5.5987 - val_accuracy: 0.1602\n",
      "Epoch 16/50\n",
      "10181/10181 [==============================] - 573s 56ms/step - loss: 5.5232 - accuracy: 0.1650 - val_loss: 5.6006 - val_accuracy: 0.1546\n",
      "Epoch 17/50\n",
      "10181/10181 [==============================] - 561s 55ms/step - loss: 5.5283 - accuracy: 0.1639 - val_loss: 5.6010 - val_accuracy: 0.1599\n",
      "Epoch 18/50\n",
      "10181/10181 [==============================] - 564s 55ms/step - loss: 5.5078 - accuracy: 0.1666 - val_loss: 5.5813 - val_accuracy: 0.1615\n",
      "Epoch 19/50\n",
      "10181/10181 [==============================] - 566s 56ms/step - loss: 5.4940 - accuracy: 0.1664 - val_loss: 5.5759 - val_accuracy: 0.1611\n",
      "Epoch 20/50\n",
      "10181/10181 [==============================] - 564s 55ms/step - loss: 5.4905 - accuracy: 0.1675 - val_loss: 5.6229 - val_accuracy: 0.1615\n",
      "Epoch 21/50\n",
      "10181/10181 [==============================] - 566s 56ms/step - loss: 5.3895 - accuracy: 0.1767 - val_loss: 5.5890 - val_accuracy: 0.1646\n",
      "Epoch 22/50\n",
      "10181/10181 [==============================] - 568s 56ms/step - loss: 5.3935 - accuracy: 0.1756 - val_loss: 5.6009 - val_accuracy: 0.1669\n",
      "Epoch 23/50\n",
      "10181/10181 [==============================] - 563s 55ms/step - loss: 5.4107 - accuracy: 0.1744 - val_loss: 5.5020 - val_accuracy: 0.1649\n",
      "Epoch 24/50\n",
      "10181/10181 [==============================] - 571s 56ms/step - loss: 5.4058 - accuracy: 0.1763 - val_loss: 5.5818 - val_accuracy: 0.1625\n",
      "Epoch 25/50\n",
      "10181/10181 [==============================] - 568s 56ms/step - loss: 5.4298 - accuracy: 0.1740 - val_loss: 5.5797 - val_accuracy: 0.1679\n",
      "Epoch 26/50\n",
      "10181/10181 [==============================] - 567s 56ms/step - loss: 5.4165 - accuracy: 0.1729 - val_loss: 5.5461 - val_accuracy: 0.1621\n",
      "Epoch 27/50\n",
      "10181/10181 [==============================] - 573s 56ms/step - loss: 5.4312 - accuracy: 0.1743 - val_loss: 5.5407 - val_accuracy: 0.1625\n",
      "Epoch 28/50\n",
      "10181/10181 [==============================] - 570s 56ms/step - loss: 5.4356 - accuracy: 0.1752 - val_loss: 5.5829 - val_accuracy: 0.1653\n",
      "Epoch 29/50\n",
      "10181/10181 [==============================] - 569s 56ms/step - loss: 5.4272 - accuracy: 0.1736 - val_loss: 5.5151 - val_accuracy: 0.1643\n",
      "Epoch 30/50\n",
      "10181/10181 [==============================] - 570s 56ms/step - loss: 5.3761 - accuracy: 0.1780 - val_loss: 5.5650 - val_accuracy: 0.1611\n",
      "Epoch 31/50\n",
      "10181/10181 [==============================] - 567s 56ms/step - loss: 5.2935 - accuracy: 0.1866 - val_loss: 5.5800 - val_accuracy: 0.1665\n",
      "Epoch 32/50\n",
      "10181/10181 [==============================] - 569s 56ms/step - loss: 5.3127 - accuracy: 0.1835 - val_loss: 5.5524 - val_accuracy: 0.1649\n",
      "Epoch 33/50\n",
      "10181/10181 [==============================] - 567s 56ms/step - loss: 5.3127 - accuracy: 0.1838 - val_loss: 5.5292 - val_accuracy: 0.1695\n",
      "Epoch 34/50\n",
      "10181/10181 [==============================] - 545s 54ms/step - loss: 5.3488 - accuracy: 0.1818 - val_loss: 5.5536 - val_accuracy: 0.1650\n",
      "Epoch 35/50\n",
      "10181/10181 [==============================] - 532s 52ms/step - loss: 5.3494 - accuracy: 0.1826 - val_loss: 5.5657 - val_accuracy: 0.1596\n",
      "Epoch 36/50\n",
      "10181/10181 [==============================] - 518s 51ms/step - loss: 5.3501 - accuracy: 0.1816 - val_loss: 5.5404 - val_accuracy: 0.1713\n",
      "Epoch 37/50\n",
      "10181/10181 [==============================] - 518s 51ms/step - loss: 5.3507 - accuracy: 0.1818 - val_loss: 5.5371 - val_accuracy: 0.1675\n",
      "Epoch 38/50\n",
      "10181/10181 [==============================] - 513s 50ms/step - loss: 5.3733 - accuracy: 0.1794 - val_loss: 5.5268 - val_accuracy: 0.1624\n",
      "Epoch 39/50\n",
      "10181/10181 [==============================] - 514s 50ms/step - loss: 5.3465 - accuracy: 0.1811 - val_loss: 5.4982 - val_accuracy: 0.1667\n",
      "Epoch 40/50\n",
      "10181/10181 [==============================] - 513s 50ms/step - loss: 5.3020 - accuracy: 0.1863 - val_loss: 5.5421 - val_accuracy: 0.1683\n",
      "Epoch 41/50\n",
      "10181/10181 [==============================] - 516s 51ms/step - loss: 5.2070 - accuracy: 0.1943 - val_loss: 5.5399 - val_accuracy: 0.1654\n",
      "Epoch 42/50\n",
      "10181/10181 [==============================] - 520s 51ms/step - loss: 5.2376 - accuracy: 0.1900 - val_loss: 5.5141 - val_accuracy: 0.1613\n",
      "Epoch 43/50\n",
      "10181/10181 [==============================] - 518s 51ms/step - loss: 5.2628 - accuracy: 0.1889 - val_loss: 5.5257 - val_accuracy: 0.1676\n",
      "Epoch 44/50\n",
      "10181/10181 [==============================] - 510s 50ms/step - loss: 5.2873 - accuracy: 0.1889 - val_loss: 5.5100 - val_accuracy: 0.1761\n",
      "Epoch 45/50\n",
      "10181/10181 [==============================] - 515s 51ms/step - loss: 5.2978 - accuracy: 0.1870 - val_loss: 5.5837 - val_accuracy: 0.1628\n",
      "Epoch 46/50\n",
      "10181/10181 [==============================] - 514s 51ms/step - loss: 5.2805 - accuracy: 0.1860 - val_loss: 5.4348 - val_accuracy: 0.1732\n",
      "Epoch 47/50\n",
      "10181/10181 [==============================] - 511s 50ms/step - loss: 5.3111 - accuracy: 0.1863 - val_loss: 5.4640 - val_accuracy: 0.1730\n",
      "Epoch 48/50\n",
      "10181/10181 [==============================] - 515s 51ms/step - loss: 5.3147 - accuracy: 0.1866 - val_loss: 5.5194 - val_accuracy: 0.1697\n",
      "Epoch 49/50\n",
      "10181/10181 [==============================] - 516s 51ms/step - loss: 5.3035 - accuracy: 0.1856 - val_loss: 5.5254 - val_accuracy: 0.1689\n",
      "Epoch 50/50\n",
      "10181/10181 [==============================] - 516s 51ms/step - loss: 5.2208 - accuracy: 0.1908 - val_loss: 5.5255 - val_accuracy: 0.1669\n"
     ]
    }
   ],
   "source": [
    "def criar_gerador(dataset):\n",
    "    while 42:\n",
    "        indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(indices)\n",
    "        for i in indices:\n",
    "            yield np.array(dataset[i][:-1]).reshape(1, -1), np.array(dataset[i][1:]).reshape(1, -1, 1)\n",
    "\n",
    "gerador = criar_gerador(dataset)\n",
    "gerador_val = criar_gerador(test_dataset)\n",
    "history = modelo.fit_generator(gerador, epochs=50, steps_per_epoch=tamanho_dataset//10, validation_data=gerador_val, validation_steps=len(test_dataset), validation_freq=1)\n",
    "model.save('LSTM-de-assis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo = tf.keras.models.load_model(CAMINHO_MODELO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nota então olha semana para viver . outro teatro ,\n",
      "vagido de justiça ! pelo banco . camilo , como\n",
      "adequada aos joão de melo , demais , não prazer\n",
      "internúncio , velho elas , meus e adelaide , lhe\n",
      "bungo e senhoras , quando eu disse escrito , forma\n",
      "abolido os seus fins . olhos . em casa da\n",
      "respondeulhes vasconcelos , compôs fora hoje os políticos . de\n",
      "iras da glória . criança mesmo , página da mãos\n",
      "cornélia bastava de a ira amigo do o ponta do\n",
      "apóstrofe do sol ! dos dizer quanto por ocasião paciência\n"
     ]
    }
   ],
   "source": [
    "def gerar_tabela_pred(pred, dicionario_indices):\n",
    "    pred_df = pd.Series(pred[0][-1])\n",
    "    pred_df = pd.DataFrame([(dicionario_indices[indice], valor) for indice, valor in pred_df.sort_values(ascending=False).iteritems()])\n",
    "    pred_df = pred_df.rename(columns={0: \"palavra\", 1: \"probabilidade\"})\n",
    "    return pred_df\n",
    "\n",
    "def predizer_palavra(modelo, frase, dicionario, dicionario_indices):\n",
    "    vetor = [[[dicionario.get(palavra, len(dicionario)-1) for palavra in frase.split()]]]\n",
    "    pred = modelo.predict(vetor)\n",
    "    tabela = gerar_tabela_pred(pred, dicionario_indices)\n",
    "    palavra = \"UNK\"\n",
    "    pontuacao =  [\".\", \",\", \"!\", \"?\"]\n",
    "    while palavra == \"UNK\" or (frase[-1] in pontuacao and palavra in pontuacao):\n",
    "        palavra = np.random.choice(tabela[\"palavra\"], p=tabela[\"probabilidade\"]/tabela[\"probabilidade\"].sum())\n",
    "    return palavra\n",
    "    \n",
    "def criar_frase(modelo, dicionario, dicionario_indices, frase_inicial=\"\", qtd_palavras=10):\n",
    "    frase = np.random.choice(list(dicionario.keys())) if frase_inicial==\"\" else frase_inicial.lower()\n",
    "    while len(frase.split()) < qtd_palavras:\n",
    "        frase += \" \" + predizer_palavra(modelo, frase, dicionario, dicionario_indices)\n",
    "    return frase\n",
    "\n",
    "for i in range(10):\n",
    "    print(criar_frase(model, dicionario, dicionario_indices, frase_inicial=\"\", qtd_palavras=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
