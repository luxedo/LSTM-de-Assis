{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90dc8b9875c680a2cadfa810793d1fdc618842a1"
   },
   "source": [
    "# LSTM com Embeddings (Feat. Machado de Assis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import List, Set, Dict, Tuple, Generator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "CAMINHO_MODELO = \"modelo.json\"\n",
    "CAMINHO_DICIONARIO = \"dicionario.json\"\n",
    "CAMINHO_DICIONARIO_INDICES = \"dicionario_indices.json\"\n",
    "EMBEDDING_UNITS = 64\n",
    "LSTM_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def criar_dicionario(palavras: List[str], minimo_palavras: int) -> (Dict[str, int], Dict[int, str], Dict[str, int]):\n",
    "    dicionario_freq = collections.Counter(palavras)\n",
    "    palavras_no_dicionario = [palavra for palavra, qtd in dicionario_freq.items() if qtd > minimo_palavras]\n",
    "    dicionario = {palavra: indice for (indice, palavra) in enumerate(palavras_no_dicionario)}\n",
    "    dicionario[\"UNK\"] = len(dicionario)\n",
    "    dicionario_indices = dict(zip(dicionario.values(), dicionario.keys()))\n",
    "    return dicionario, dicionario_indices, dicionario_freq\n",
    "df_obras = pd.read_csv(\"./obras_machado_de_assis.csv\")\n",
    "df_obras = df_obras[df_obras[\"categoria\"] != \"tradução\"]\n",
    "\n",
    "minimo_palavras_frase = 2\n",
    "dataset = pd.Series(np.concatenate(df_obras[\"texto\"].str.replace(\"\\n+\", \" \").str.replace(\"\\.+\", \".\").str.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\").values))\n",
    "dataset = dataset.str.replace(\"[^\\w\\s\\d]\", \"\").str.strip().str.lower()\n",
    "dataset = dataset[dataset.apply(lambda row: len(row.split())) > minimo_palavras_frase].reset_index(drop=True)\n",
    "palavras = np.concatenate(dataset.apply(lambda row: row.split()))\n",
    "\n",
    "minimo_palavras_dicionario = 2\n",
    "dicionario, dicionario_indices, dicionario_freq = criar_dicionario(palavras, minimo_palavras_dicionario)\n",
    "dataset = dataset.apply(lambda row: [dicionario.get(palavra, len(dicionario)-1) for palavra in row.split()])\n",
    "tamanho_dicionario = len(dicionario)\n",
    "tamanho_dataset = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "d66f8422cd7f82e209b6b9b9eaccff46f2f7299c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases no dataset: 98366\n",
      "Total de palavras: 1698151\n",
      "Palavras no dicionário: 28406\n",
      "Média de palavras por frase: 17.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Frases no dataset: {0}\\nTotal de palavras: {1}\\nPalavras no dicionário: {2}\\nMédia de palavras por frase: {3}\".format(len(dataset), len(palavras), len(dicionario), round(len(palavras)/len(dataset), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "814dab319048ae052b210dfd592c9f23be3afe08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            [0, 1, 2]\n",
       "1                                         [3, 4, 5, 6]\n",
       "2              [7, 8, 9, 10, 2, 11, 12, 13, 12, 14, 2]\n",
       "3    [3, 15, 16, 17, 15, 18, 19, 20, 2, 21, 22, 23,...\n",
       "4    [49, 50, 51, 52, 53, 54, 35, 29, 22, 55, 2, 56...\n",
       "dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "d4617c21520fdb932ebb99cb5f96c797e17258cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "entrada (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 64)          1817984   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "saida (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,916,929\n",
      "Trainable params: 1,916,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def criar_modelo(dim_entrada, dim_embedding, dim_lstm):\n",
    "    entrada = tf.keras.layers.Input((None,), name=\"entrada\")\n",
    "    camada_embedding = tf.keras.layers.Embedding(dim_entrada, dim_embedding, name='embedding')\n",
    "    camada_contexto = camada_embedding(entrada)\n",
    "    lstm, estado_h, estado_c = tf.keras.layers.LSTM(dim_lstm, name=\"lstm\", return_states=True)(camada_contexto)\n",
    "    camada_saida = tf.keras.layers.Dense(1,name=\"saida\", activation=\"sigmoid\")(lstm)\n",
    "    \n",
    "    modelo = tf.keras.models.Model(inputs=entrada, outputs=camada_saida)\n",
    "    modelo.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"acc\"])\n",
    "    \n",
    "    modelo_estado = tf.keras.models.Model(inputs=entrada, outputs=[estado_h, estado_c])\n",
    "    return modelo\n",
    "\n",
    "def criar_modelo_preditivo(dim_entrada, modelo_estado):\n",
    "    entrada_preditivo = tf.keras.layers.Input((1,), name=\"entrada_preditivo\")\n",
    "\n",
    "# modelo = criar_modelo(dim_entrada=tamanho_dicionario, dim_embedding=EMBEDDING_UNITS, dim_lstm=LSTM_UNITS)\n",
    "# modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b64704bd07796aa54c2c502737c10a244051602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 73595/196732 [==========>...................] - ETA: 1:11:40 - loss: 0.2590 - acc: 0.8792"
     ]
    }
   ],
   "source": [
    "def criar_gerador(dataset, tamanho_dicionario):\n",
    "    batchX = []\n",
    "    batchY = []\n",
    "    for idx, d in enumerate(dataset):\n",
    "        positivo = np.array(d)\n",
    "        negativo = np.array(d)\n",
    "        batchX.append(positivo)\n",
    "        batchY.append(1)\n",
    "        np.random.shuffle(negativo)\n",
    "        batchX.append(negativo)\n",
    "        batchY.append(0)\n",
    "    ordem = np.arange(len(batchY))\n",
    "    np.random.shuffle(ordem)\n",
    "    batchX = np.array(batchX)[ordem]\n",
    "    batchY = np.array(batchY)[ordem].reshape((-1, 1))\n",
    "    for x, y in zip(batchX, batchY):\n",
    "        yield (x.reshape(1, -1), y)\n",
    "\n",
    "\n",
    "gerador = criar_gerador(dataset, tamanho_dicionario)\n",
    "modelo.fit_generator(gerador, epochs=1, steps_per_epoch=2*tamanho_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_uuid": "ff8f3dc68f773e890476acc45f4794aeb8a0986e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.014203823552459654, 0.997]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.evaluate_generator(gerador, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_uuid": "832b0d19580748b96552e766b7ba6066146456a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('poesias', array([[0.9998054]], dtype=float32)), ('de', array([[0.9995316]], dtype=float32)), ('a', array([[0.99953747]], dtype=float32)), ('zaluar', array([[0.99983716]], dtype=float32)), ('garnier', array([[0.9998442]], dtype=float32)), ('editor', array([[0.9997458]], dtype=float32)), ('1863', array([[0.9999052]], dtype=float32)), ('dois', array([[0.99989104]], dtype=float32)), ('motivos', array([[0.99992335]], dtype=float32)), ('me', array([[0.9995493]], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "frase = \"Eu gostaria de comprar um\".lower()\n",
    "probs = []\n",
    "for palavra, indice in dicionario.items():\n",
    "    frase2 = frase + \" \" + palavra\n",
    "    vetor = np.array([[dicionario.get(palavra, tamanho_dicionario-1) for palavra in frase2.split()]])\n",
    "    probs.append((palavra, modelo.predict(vetor)))\n",
    "print(probs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_uuid": "8afd5815aaa93c3fa398ed3ff799ebbe7ce55565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', array([[0.9999726]], dtype=float32)),\n",
       " ('mundo', array([[0.9999697]], dtype=float32)),\n",
       " ('terra', array([[0.99996877]], dtype=float32)),\n",
       " ('coisa', array([[0.99996865]], dtype=float32)),\n",
       " ('casa', array([[0.9999685]], dtype=float32)),\n",
       " ('mãe', array([[0.99996805]], dtype=float32)),\n",
       " ('futuro', array([[0.9999678]], dtype=float32)),\n",
       " ('quê', array([[0.9999677]], dtype=float32)),\n",
       " ('tarde', array([[0.999967]], dtype=float32)),\n",
       " ('homens', array([[0.99996686]], dtype=float32))]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(probs, key=lambda x: -x[1][0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "150b3463f6979098c59dbf5af9e3e8af5b837c2a"
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    hidden = modelo.reset_states()\n",
    "    for (batch, (input, target)) in enumerate(ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, hidden = modelo(input, hidden)\n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    "            grads = tape.gradient(loss, modelo.variables)\n",
    "            optimizer.apply_gradients(zip(grads, modelo.variables))\n",
    "            if batch % 1000 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80d4bc221c53898553405128e1aa405fd440e644"
   },
   "outputs": [],
   "source": [
    "start_string = \"Eduardo viajou para o que\"\n",
    "\n",
    "input_eval = [dicionario.get(string) for string in start_string.lower().split()]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "text_generated = ''\n",
    "\n",
    "hidden = [tf.zeros((1, LSTM_UNITS))]\n",
    "\n",
    "predictions, hidden = modelo(input_eval, hidden)\n",
    "\n",
    "predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "\n",
    "text_generated += \" \" + dicionario_indices[predicted_id]\n",
    "\n",
    "print(start_string + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34583528ee2d0fec751c4c68fbc4c648c95c10b5"
   },
   "outputs": [],
   "source": [
    "def criar_gerador(dataset, dicionario):\n",
    "    for frase in dataset:\n",
    "        \n",
    "#         yield (np.array([frase[:-1]]), np.array([frase[-1]]))\n",
    "        \n",
    "next(criar_gerador(dataset, dicionario))\n",
    "\n",
    "# gerador = criar_gerador(dataset, dicionario, modelo_embedding)\n",
    "# modelo.fit_generator(gerador, epochs=1, steps_per_epoch=len(dataset)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfd6f2912b49223efa576c024b93e94536ee2df0"
   },
   "outputs": [],
   "source": [
    "embeddings = modelo_embedding.predict(np.arange(len(dicionario)))\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "834275db0f35e8398a2c067a80d5e58d015a6687"
   },
   "outputs": [],
   "source": [
    "frase = \"a carne mais barata do mercado é a carne tendo\".lower()\n",
    "entrada = [dicionario.get(palavra, len(dicionario)-1) for palavra in frase.split()]\n",
    "vec = modelo.predict(entrada)\n",
    "[dicionario_indices[indice] for indice in distancia_coseno(embeddings, vec).argmin(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a2c685a7528f23496fcd84bda90c9bf223577a3"
   },
   "outputs": [],
   "source": [
    "[dicionario_indices[indice] for indice in (np.dot(vec, embeddings.T)/vec.sum(axis=1).reshape((-1, 1))/embeddings.sum(axis=1)).argmin(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c1d9335dfaaf3bbe1e41394806fb653f976fbd3"
   },
   "outputs": [],
   "source": [
    "def distancia_coseno(embeddings, vetores):\n",
    "    return (np.dot(embeddings, vec.T)/(vec**2).sum(axis=1)**0.5)/((embeddings**2).sum(axis=1)**0.5).reshape((-1, 1))\n",
    "# np.dot(vec, embeddings.T)/vec.sum(axis=1).reshape((-1, 1))/embeddings.sum(axis=1)\n",
    "x\n",
    "#  (vec**2).sum(axis=1)/embeddings.sum(axis=1).reshape((-1, 1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4855ec44fafd94683551271221c41b91fe784908"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b04a04fecf6c62268c5e901aa6da247588e39206"
   },
   "outputs": [],
   "source": [
    "# np.argmax((embeddings - vec).sum(axis=0))\n",
    "np.array([embeddings - v for v in vec]).sum(axis=2).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40df63da1b468961bff4db9e7de011708b99d0ca"
   },
   "outputs": [],
   "source": [
    "dicionario_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53a755cc2131b1b38e792ee4a58e93f524ea7b85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25fc97309a8d3970edd5df286726184b7c904265"
   },
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size) \n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        output, states = self.gru(inputs, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, states\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "modelo = Model(len(dicionario), EMBEDDINGS, LSTM_UNITS, BATCH_SIZE)\n",
    "# modelo.build((10,))\n",
    "# modelo.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"acc\"])\n",
    "# modelo, modelo_embedding = criar_modelo(len(dicionario), EMBEDDINGS)\n",
    "# modelo_embedding.summary()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(labels, logits):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "X, Y = [], []\n",
    "for d in dataset:\n",
    "    for i in range(1, len(d)):\n",
    "        X.append(d[i-1])\n",
    "        Y.append(d[i])\n",
    "\n",
    "X, Y = np.array(X).reshape((-1, 1)), np.array(Y).reshape(-1, 1)\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "ds = ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    hidden = modelo.reset_states()\n",
    "    for (batch, (input, target)) in enumerate(ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, hidden = modelo(input, hidden)\n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    "            grads = tape.gradient(loss, modelo.variables)\n",
    "            optimizer.apply_gradients(zip(grads, modelo.variables))\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss{:.4f}'.format(epoch + 1, batch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6ff08562f33cd943118cb7759829c0b4bdd2491"
   },
   "outputs": [],
   "source": [
    "sorted(get_preds(\"não\"), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42181896387eee03b84cf0dcfc6eb420b3f40379"
   },
   "outputs": [],
   "source": [
    "get_preds(\"quatro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2394b835e87662835d60415d584d66bcdf686c55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
