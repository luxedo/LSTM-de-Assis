{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import List, Set, Dict, Tuple, Generator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "CAMINHO_MODELO = \"modelo.json\"\n",
    "CAMINHO_DICIONARIO = \"dicionario.json\"\n",
    "CAMINHO_DICIONARIO_INDICES = \"dicionario_indices.json\"\n",
    "EMBEDDING_UNITS = 128\n",
    "LSTM_UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_dicionario(palavras: List[str], minimo_palavras: int) -> (Dict[str, int], Dict[int, str], Dict[str, int]):\n",
    "    dicionario_freq = collections.Counter(palavras)\n",
    "    palavras_no_dicionario = [palavra for palavra, qtd in dicionario_freq.items() if qtd > minimo_palavras]\n",
    "    dicionario = {palavra: indice for (indice, palavra) in enumerate(palavras_no_dicionario)}\n",
    "    dicionario[\"UNK\"] = len(dicionario)\n",
    "    dicionario_indices = dict(zip(dicionario.values(), dicionario.keys()))\n",
    "    return dicionario, dicionario_indices, dicionario_freq\n",
    "df_obras = pd.read_csv(\"./obras_machado_de_assis.csv\")\n",
    "df_obras = df_obras[df_obras[\"categoria\"] != \"tradução\"]\n",
    "\n",
    "minimo_palavras_frase = 2\n",
    "dataset = pd.Series(np.concatenate(df_obras[\"texto\"].str.replace(\"\\n+\", \" \").str.replace(\"\\.+\", \".\").str.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\").values))\n",
    "dataset = dataset.str.replace(\"[^\\w\\s\\d]\", \"\").str.strip().str.lower()\n",
    "dataset = dataset[dataset.apply(lambda row: len(row.split())) > minimo_palavras_frase].reset_index(drop=True)\n",
    "palavras = np.concatenate(dataset.apply(lambda row: row.split()))\n",
    "\n",
    "minimo_palavras_dicionario = 2\n",
    "dicionario, dicionario_indices, dicionario_freq = criar_dicionario(palavras, minimo_palavras_dicionario)\n",
    "dataset = dataset.apply(lambda row: [dicionario.get(palavra, len(dicionario)-1) for palavra in row.split()])\n",
    "tamanho_dicionario = len(dicionario)\n",
    "tamanho_dataset = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1 Batch 0 Loss10.2544\n",
      "Epoch 1 Batch 100 Loss7.8267\n",
      "Epoch 1 Batch 200 Loss8.0981\n",
      "Epoch 1 Batch 300 Loss8.1649\n",
      "Epoch 1 Batch 400 Loss7.0558\n",
      "Epoch 1 Batch 500 Loss7.4419\n",
      "Epoch 1 Batch 600 Loss6.8362\n",
      "Epoch 1 Batch 700 Loss6.2766\n",
      "Epoch 1 Batch 800 Loss6.5017\n",
      "Epoch 1 Batch 900 Loss6.3314\n",
      "Epoch 1 Batch 1000 Loss5.9891\n",
      "Epoch 1 Batch 1100 Loss7.2984\n",
      "Epoch 1 Batch 1200 Loss6.6151\n",
      "Epoch 1 Batch 1300 Loss7.4415\n",
      "Epoch 1 Batch 1400 Loss6.7402\n",
      "Epoch 1 Batch 1500 Loss6.3960\n",
      "Epoch 1 Batch 1600 Loss7.3402\n",
      "Epoch 1 Batch 1700 Loss7.1340\n",
      "Epoch 1 Batch 1800 Loss6.6472\n",
      "Epoch 1 Batch 1900 Loss6.8760\n",
      "Epoch 1 Batch 2000 Loss6.3845\n",
      "Epoch 1 Batch 2100 Loss6.4398\n",
      "Epoch 1 Batch 2200 Loss6.4507\n",
      "Epoch 1 Batch 2300 Loss6.2884\n",
      "Epoch 1 Batch 2400 Loss7.1156\n",
      "Epoch 1 Batch 2500 Loss6.3470\n",
      "Epoch 1 Batch 2600 Loss6.1039\n",
      "Epoch 1 Batch 2700 Loss6.3864\n",
      "Epoch 1 Batch 2800 Loss6.9723\n",
      "Epoch 1 Batch 2900 Loss6.7774\n",
      "Epoch 1 Batch 3000 Loss6.6923\n",
      "Epoch 1 Batch 3100 Loss6.2856\n",
      "Epoch 1 Batch 3200 Loss5.8234\n",
      "Epoch 1 Batch 3300 Loss6.0184\n",
      "Epoch 1 Batch 3400 Loss6.6287\n",
      "Epoch 1 Batch 3500 Loss5.9222\n",
      "Epoch 1 Batch 3600 Loss6.2554\n",
      "Epoch 1 Batch 3700 Loss5.5730\n",
      "Epoch 1 Batch 3800 Loss6.6572\n",
      "Epoch 1 Batch 3900 Loss5.7286\n",
      "Epoch 1 Batch 4000 Loss5.6616\n",
      "Epoch 1 Batch 4100 Loss6.4521\n",
      "Epoch 1 Batch 4200 Loss6.3037\n",
      "Epoch 1 Batch 4300 Loss6.2253\n",
      "Epoch 1 Batch 4400 Loss5.5702\n",
      "Epoch 1 Batch 4500 Loss5.3458\n",
      "Epoch 1 Batch 4600 Loss5.9521\n",
      "Epoch 1 Batch 4700 Loss5.5696\n",
      "Epoch 1 Batch 4800 Loss6.3758\n",
      "Epoch 1 Batch 4900 Loss5.1964\n",
      "Epoch 1 Batch 5000 Loss6.4715\n",
      "Epoch 1 Batch 5100 Loss6.0630\n",
      "Epoch 1 Batch 5200 Loss7.1300\n",
      "Epoch 1 Batch 5300 Loss5.7870\n",
      "Epoch 1 Batch 5400 Loss5.8522\n",
      "Epoch 1 Batch 5500 Loss5.8125\n",
      "Epoch 1 Batch 5600 Loss6.6477\n",
      "Epoch 1 Batch 5700 Loss6.4648\n",
      "Epoch 1 Batch 5800 Loss5.3295\n",
      "Epoch 1 Batch 5900 Loss5.4181\n",
      "Epoch 1 Batch 6000 Loss5.9605\n",
      "Epoch 1 Batch 6100 Loss6.3315\n",
      "Epoch 1 Batch 6200 Loss6.0092\n",
      "Epoch 1 Batch 6300 Loss5.2044\n",
      "Epoch 1 Batch 6400 Loss5.8680\n",
      "Epoch 1 Batch 6500 Loss5.7335\n",
      "Epoch 1 Batch 6600 Loss6.9512\n",
      "Epoch 1 Batch 6700 Loss6.7911\n",
      "Epoch 1 Batch 6800 Loss6.5221\n",
      "Epoch 1 Batch 6900 Loss6.8390\n",
      "Epoch 1 Batch 7000 Loss7.3332\n",
      "Epoch 1 Batch 7100 Loss7.4177\n",
      "Epoch 1 Batch 7200 Loss7.2242\n",
      "Epoch 1 Batch 7300 Loss5.9735\n",
      "Epoch 1 Batch 7400 Loss6.3651\n",
      "Epoch 1 Batch 7500 Loss6.5459\n",
      "Epoch 1 Batch 7600 Loss7.2727\n",
      "Epoch 1 Batch 7700 Loss6.4478\n",
      "Epoch 1 Batch 7800 Loss5.8745\n",
      "Epoch 1 Batch 7900 Loss5.7866\n",
      "Epoch 1 Batch 8000 Loss6.2908\n",
      "Epoch 1 Batch 8100 Loss6.2883\n",
      "Epoch 1 Batch 8200 Loss6.7127\n",
      "Epoch 1 Batch 8300 Loss6.3869\n",
      "Epoch 1 Batch 8400 Loss6.3784\n",
      "Epoch 1 Batch 8500 Loss6.7520\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cd8406b8bf97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1129\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5307\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5308\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5309\u001b[0;31m         \"transpose_b\", transpose_b)\n\u001b[0m\u001b[1;32m   5310\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5311\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size) \n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        output, states = self.gru(inputs, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, states\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "modelo = Model(len(dicionario), EMBEDDING_UNITS, LSTM_UNITS, BATCH_SIZE)\n",
    "# modelo.build((10,))\n",
    "# modelo.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"acc\"])\n",
    "# modelo, modelo_embedding = criar_modelo(len(dicionario), EMBEDDINGS)\n",
    "# modelo_embedding.summary()\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "def loss_function(labels, logits):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "X, Y = [], []\n",
    "for d in dataset:\n",
    "    for i in range(1, len(d)):\n",
    "        X.append(d[i-1])\n",
    "        Y.append(d[i])\n",
    "\n",
    "X, Y = np.array(X).reshape((-1, 1)), np.array(Y).reshape(-1, 1)\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "ds = ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    hidden = modelo.reset_states()\n",
    "    for (batch, (input, target)) in enumerate(ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, hidden = modelo(input, hidden)\n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    "            grads = tape.gradient(loss, modelo.variables)\n",
    "            optimizer.apply_gradients(zip(grads, modelo.variables))\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss{:.4f}'.format(epoch + 1, batch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eduardo viajou não sei que o\n"
     ]
    }
   ],
   "source": [
    "start_string = \"Eduardo viajou não sei que\"\n",
    "\n",
    "input_eval = [dicionario.get(string) for string in start_string.lower().split()]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "text_generated = ''\n",
    "\n",
    "hidden = [tf.zeros((1, LSTM_UNITS))]\n",
    "\n",
    "predictions, hidden = modelo(input_eval, hidden)\n",
    "\n",
    "predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "\n",
    "text_generated += \" \" + dicionario_indices[predicted_id]\n",
    "\n",
    "print(start_string + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss6.7043\n",
      "Epoch 1 Batch 100 Loss5.9692\n",
      "Epoch 1 Batch 200 Loss6.0786\n",
      "Epoch 1 Batch 300 Loss6.6230\n",
      "Epoch 1 Batch 400 Loss5.5928\n",
      "Epoch 1 Batch 500 Loss6.2438\n",
      "Epoch 1 Batch 600 Loss5.8269\n",
      "Epoch 1 Batch 700 Loss5.2772\n",
      "Epoch 1 Batch 800 Loss5.5680\n",
      "Epoch 1 Batch 900 Loss5.5921\n",
      "Epoch 1 Batch 1000 Loss5.0857\n",
      "Epoch 1 Batch 1100 Loss6.2973\n",
      "Epoch 1 Batch 1200 Loss5.6218\n",
      "Epoch 1 Batch 1300 Loss6.6073\n",
      "Epoch 1 Batch 1400 Loss5.7490\n",
      "Epoch 1 Batch 1500 Loss5.8344\n",
      "Epoch 1 Batch 1600 Loss6.6077\n",
      "Epoch 1 Batch 1700 Loss6.1136\n",
      "Epoch 1 Batch 1800 Loss5.9632\n",
      "Epoch 1 Batch 1900 Loss5.8360\n",
      "Epoch 1 Batch 2000 Loss5.6364\n",
      "Epoch 1 Batch 2100 Loss5.9290\n",
      "Epoch 1 Batch 2200 Loss5.8303\n",
      "Epoch 1 Batch 2300 Loss5.8793\n",
      "Epoch 1 Batch 2400 Loss6.1046\n",
      "Epoch 1 Batch 2500 Loss5.6369\n",
      "Epoch 1 Batch 2600 Loss5.7357\n",
      "Epoch 1 Batch 2700 Loss5.7923\n",
      "Epoch 1 Batch 2800 Loss6.3765\n",
      "Epoch 1 Batch 2900 Loss6.2885\n",
      "Epoch 1 Batch 3000 Loss6.0760\n",
      "Epoch 1 Batch 3100 Loss5.7663\n",
      "Epoch 1 Batch 3200 Loss5.3522\n",
      "Epoch 1 Batch 3300 Loss5.7537\n",
      "Epoch 1 Batch 3400 Loss6.0677\n",
      "Epoch 1 Batch 3500 Loss5.5274\n",
      "Epoch 1 Batch 3600 Loss5.7363\n",
      "Epoch 1 Batch 3700 Loss5.1946\n",
      "Epoch 1 Batch 3800 Loss6.1858\n",
      "Epoch 1 Batch 3900 Loss5.5166\n",
      "Epoch 1 Batch 4000 Loss5.3705\n",
      "Epoch 1 Batch 4100 Loss5.8703\n",
      "Epoch 1 Batch 4200 Loss5.5894\n",
      "Epoch 1 Batch 4300 Loss5.7491\n",
      "Epoch 1 Batch 4400 Loss5.1943\n",
      "Epoch 1 Batch 4500 Loss5.0648\n",
      "Epoch 1 Batch 4600 Loss5.6200\n",
      "Epoch 1 Batch 4700 Loss5.2782\n",
      "Epoch 1 Batch 4800 Loss5.8145\n",
      "Epoch 1 Batch 4900 Loss4.9536\n",
      "Epoch 1 Batch 5000 Loss5.9390\n",
      "Epoch 1 Batch 5100 Loss5.6450\n",
      "Epoch 1 Batch 5200 Loss6.4640\n",
      "Epoch 1 Batch 5300 Loss5.4841\n",
      "Epoch 1 Batch 5400 Loss5.4839\n",
      "Epoch 1 Batch 5500 Loss5.3592\n",
      "Epoch 1 Batch 5600 Loss6.1299\n",
      "Epoch 1 Batch 5700 Loss5.8705\n",
      "Epoch 1 Batch 5800 Loss5.0460\n",
      "Epoch 1 Batch 5900 Loss5.1817\n",
      "Epoch 1 Batch 6000 Loss5.6346\n",
      "Epoch 1 Batch 6100 Loss5.8934\n",
      "Epoch 1 Batch 6200 Loss5.5861\n",
      "Epoch 1 Batch 6300 Loss4.9401\n",
      "Epoch 1 Batch 6400 Loss5.6421\n",
      "Epoch 1 Batch 6500 Loss5.3783\n",
      "Epoch 1 Batch 6600 Loss6.3607\n",
      "Epoch 1 Batch 6700 Loss6.2504\n",
      "Epoch 1 Batch 6800 Loss6.1334\n",
      "Epoch 1 Batch 6900 Loss6.3258\n",
      "Epoch 1 Batch 7000 Loss6.7279\n",
      "Epoch 1 Batch 7100 Loss6.8286\n",
      "Epoch 1 Batch 7200 Loss6.6727\n",
      "Epoch 1 Batch 7300 Loss5.7550\n",
      "Epoch 1 Batch 7400 Loss5.9536\n",
      "Epoch 1 Batch 7500 Loss6.0355\n",
      "Epoch 1 Batch 7600 Loss5.9799\n",
      "Epoch 1 Batch 7700 Loss6.0243\n",
      "Epoch 1 Batch 7800 Loss5.6825\n",
      "Epoch 1 Batch 7900 Loss5.4677\n",
      "Epoch 1 Batch 8000 Loss5.9033\n",
      "Epoch 1 Batch 8100 Loss6.0277\n",
      "Epoch 1 Batch 8200 Loss6.3668\n",
      "Epoch 1 Batch 8300 Loss5.8915\n",
      "Epoch 1 Batch 8400 Loss5.9866\n",
      "Epoch 1 Batch 8500 Loss6.3833\n",
      "Epoch 1 Batch 8600 Loss6.1800\n",
      "Epoch 1 Batch 8700 Loss6.9838\n",
      "Epoch 1 Batch 8800 Loss6.3834\n",
      "Epoch 1 Batch 8900 Loss6.2015\n",
      "Epoch 1 Batch 9000 Loss6.1818\n",
      "Epoch 1 Batch 9100 Loss5.4742\n",
      "Epoch 1 Batch 9200 Loss6.4710\n",
      "Epoch 1 Batch 9300 Loss6.7391\n",
      "Epoch 1 Batch 9400 Loss6.0280\n",
      "Epoch 1 Batch 9500 Loss5.9071\n",
      "Epoch 1 Batch 9600 Loss5.5824\n",
      "Epoch 1 Batch 9700 Loss6.6974\n",
      "Epoch 1 Batch 9800 Loss5.9297\n",
      "Epoch 1 Batch 9900 Loss6.1851\n",
      "Epoch 1 Batch 10000 Loss6.7132\n",
      "Epoch 1 Batch 10100 Loss6.4260\n",
      "Epoch 1 Batch 10200 Loss5.8717\n",
      "Epoch 1 Batch 10300 Loss5.6725\n",
      "Epoch 1 Batch 10400 Loss5.8687\n",
      "Epoch 1 Batch 10500 Loss6.0312\n",
      "Epoch 1 Batch 10600 Loss6.1601\n",
      "Epoch 1 Batch 10700 Loss6.0127\n",
      "Epoch 1 Batch 10800 Loss6.3535\n",
      "Epoch 1 Batch 10900 Loss6.3236\n",
      "Epoch 1 Batch 11000 Loss6.0340\n",
      "Epoch 1 Batch 11100 Loss6.2616\n",
      "Epoch 1 Batch 11200 Loss6.0355\n",
      "Epoch 1 Batch 11300 Loss5.6460\n",
      "Epoch 1 Batch 11400 Loss5.3408\n",
      "Epoch 1 Batch 11500 Loss5.5431\n",
      "Epoch 1 Batch 11600 Loss6.1501\n",
      "Epoch 1 Batch 11700 Loss6.1734\n",
      "Epoch 1 Batch 11800 Loss6.3308\n",
      "Epoch 1 Batch 11900 Loss5.9714\n",
      "Epoch 1 Batch 12000 Loss6.3015\n",
      "Epoch 1 Batch 12100 Loss5.7373\n",
      "Epoch 1 Batch 12200 Loss5.7916\n",
      "Epoch 1 Batch 12300 Loss5.5185\n",
      "Epoch 1 Batch 12400 Loss5.9892\n",
      "Epoch 1 Batch 12500 Loss5.9978\n",
      "Epoch 1 Batch 12600 Loss6.0529\n",
      "Epoch 1 Batch 12700 Loss6.4544\n",
      "Epoch 1 Batch 12800 Loss6.6365\n",
      "Epoch 1 Batch 12900 Loss6.5026\n",
      "Epoch 1 Batch 13000 Loss6.3488\n",
      "Epoch 1 Batch 13100 Loss6.8043\n",
      "Epoch 1 Batch 13200 Loss6.1561\n",
      "Epoch 1 Batch 13300 Loss5.6575\n",
      "Epoch 1 Batch 13400 Loss6.3507\n",
      "Epoch 1 Batch 13500 Loss5.2601\n",
      "Epoch 1 Batch 13600 Loss6.8695\n",
      "Epoch 1 Batch 13700 Loss5.8385\n",
      "Epoch 1 Batch 13800 Loss5.9580\n",
      "Epoch 1 Batch 13900 Loss6.0660\n",
      "Epoch 1 Batch 14000 Loss5.6416\n",
      "Epoch 1 Batch 14100 Loss6.0500\n",
      "Epoch 1 Batch 14200 Loss5.7378\n",
      "Epoch 1 Batch 14300 Loss5.9281\n",
      "Epoch 1 Batch 14400 Loss6.7102\n",
      "Epoch 1 Batch 14500 Loss5.9557\n",
      "Epoch 1 Batch 14600 Loss6.0198\n",
      "Epoch 1 Batch 14700 Loss6.3151\n",
      "Epoch 1 Batch 14800 Loss5.9056\n",
      "Epoch 1 Batch 14900 Loss6.2215\n",
      "Epoch 1 Batch 15000 Loss6.4413\n",
      "Epoch 1 Batch 15100 Loss6.0177\n",
      "Epoch 1 Batch 15200 Loss5.2981\n",
      "Epoch 1 Batch 15300 Loss6.2671\n",
      "Epoch 1 Batch 15400 Loss5.7357\n",
      "Epoch 1 Batch 15500 Loss5.6866\n",
      "Epoch 1 Batch 15600 Loss5.8318\n",
      "Epoch 1 Batch 15700 Loss5.2831\n",
      "Epoch 1 Batch 15800 Loss6.4514\n",
      "Epoch 1 Batch 15900 Loss5.2231\n",
      "Epoch 2 Batch 0 Loss6.3660\n",
      "Epoch 2 Batch 100 Loss5.7392\n",
      "Epoch 2 Batch 200 Loss5.7740\n",
      "Epoch 2 Batch 300 Loss6.6783\n",
      "Epoch 2 Batch 400 Loss5.5325\n",
      "Epoch 2 Batch 500 Loss6.0884\n",
      "Epoch 2 Batch 600 Loss5.8046\n",
      "Epoch 2 Batch 700 Loss5.0411\n",
      "Epoch 2 Batch 800 Loss5.4313\n",
      "Epoch 2 Batch 900 Loss5.4081\n",
      "Epoch 2 Batch 1000 Loss4.6513\n",
      "Epoch 2 Batch 1100 Loss6.1494\n",
      "Epoch 2 Batch 1200 Loss5.4599\n",
      "Epoch 2 Batch 1300 Loss6.2056\n",
      "Epoch 2 Batch 1400 Loss5.4328\n",
      "Epoch 2 Batch 1500 Loss5.6593\n",
      "Epoch 2 Batch 1600 Loss6.2579\n",
      "Epoch 2 Batch 1700 Loss5.9787\n",
      "Epoch 2 Batch 1800 Loss5.7766\n",
      "Epoch 2 Batch 1900 Loss5.5940\n",
      "Epoch 2 Batch 2000 Loss5.3527\n",
      "Epoch 2 Batch 2100 Loss5.7675\n",
      "Epoch 2 Batch 2200 Loss5.5673\n",
      "Epoch 2 Batch 2300 Loss5.6600\n",
      "Epoch 2 Batch 2400 Loss5.7349\n",
      "Epoch 2 Batch 2500 Loss5.3446\n",
      "Epoch 2 Batch 2600 Loss5.6329\n",
      "Epoch 2 Batch 2700 Loss5.7023\n",
      "Epoch 2 Batch 2800 Loss6.2805\n",
      "Epoch 2 Batch 2900 Loss6.2101\n",
      "Epoch 2 Batch 3000 Loss5.7844\n",
      "Epoch 2 Batch 3100 Loss5.6557\n",
      "Epoch 2 Batch 3200 Loss5.2022\n",
      "Epoch 2 Batch 3300 Loss5.5755\n",
      "Epoch 2 Batch 3400 Loss5.8945\n",
      "Epoch 2 Batch 3500 Loss5.3499\n",
      "Epoch 2 Batch 3600 Loss5.4769\n",
      "Epoch 2 Batch 3700 Loss5.0184\n",
      "Epoch 2 Batch 3800 Loss6.1556\n",
      "Epoch 2 Batch 3900 Loss5.3269\n",
      "Epoch 2 Batch 4000 Loss5.2838\n",
      "Epoch 2 Batch 4100 Loss5.6791\n",
      "Epoch 2 Batch 4200 Loss5.5053\n",
      "Epoch 2 Batch 4300 Loss5.6393\n",
      "Epoch 2 Batch 4400 Loss5.0272\n",
      "Epoch 2 Batch 4500 Loss5.0684\n",
      "Epoch 2 Batch 4600 Loss5.4706\n",
      "Epoch 2 Batch 4700 Loss5.2061\n",
      "Epoch 2 Batch 4800 Loss5.6131\n",
      "Epoch 2 Batch 4900 Loss4.8683\n",
      "Epoch 2 Batch 5000 Loss5.7413\n",
      "Epoch 2 Batch 5100 Loss5.5595\n",
      "Epoch 2 Batch 5200 Loss6.3081\n",
      "Epoch 2 Batch 5300 Loss5.3460\n",
      "Epoch 2 Batch 5400 Loss5.3328\n",
      "Epoch 2 Batch 5500 Loss5.2022\n",
      "Epoch 2 Batch 5600 Loss5.9212\n",
      "Epoch 2 Batch 5700 Loss5.6320\n",
      "Epoch 2 Batch 5800 Loss4.8667\n",
      "Epoch 2 Batch 5900 Loss5.1309\n",
      "Epoch 2 Batch 6000 Loss5.4312\n",
      "Epoch 2 Batch 6100 Loss5.8485\n",
      "Epoch 2 Batch 6200 Loss5.3896\n",
      "Epoch 2 Batch 6300 Loss4.7866\n",
      "Epoch 2 Batch 6400 Loss5.4705\n",
      "Epoch 2 Batch 6500 Loss5.1892\n",
      "Epoch 2 Batch 6600 Loss6.2173\n",
      "Epoch 2 Batch 6700 Loss6.1774\n",
      "Epoch 2 Batch 6800 Loss6.0784\n",
      "Epoch 2 Batch 6900 Loss6.2417\n",
      "Epoch 2 Batch 7000 Loss6.5996\n",
      "Epoch 2 Batch 7100 Loss6.7598\n",
      "Epoch 2 Batch 7200 Loss6.4502\n",
      "Epoch 2 Batch 7300 Loss5.6749\n",
      "Epoch 2 Batch 7400 Loss5.9630\n",
      "Epoch 2 Batch 7500 Loss5.9602\n",
      "Epoch 2 Batch 7600 Loss5.4774\n",
      "Epoch 2 Batch 7700 Loss5.8577\n",
      "Epoch 2 Batch 7800 Loss5.6912\n",
      "Epoch 2 Batch 7900 Loss5.4012\n",
      "Epoch 2 Batch 8000 Loss5.7785\n",
      "Epoch 2 Batch 8100 Loss5.9057\n",
      "Epoch 2 Batch 8200 Loss6.1192\n",
      "Epoch 2 Batch 8300 Loss5.7792\n",
      "Epoch 2 Batch 8400 Loss5.8092\n",
      "Epoch 2 Batch 8500 Loss6.1533\n",
      "Epoch 2 Batch 8600 Loss5.9500\n",
      "Epoch 2 Batch 8700 Loss6.4043\n",
      "Epoch 2 Batch 8800 Loss6.1503\n",
      "Epoch 2 Batch 8900 Loss5.8493\n",
      "Epoch 2 Batch 9000 Loss5.7778\n",
      "Epoch 2 Batch 9100 Loss5.3726\n",
      "Epoch 2 Batch 9200 Loss6.0754\n",
      "Epoch 2 Batch 9300 Loss6.2589\n",
      "Epoch 2 Batch 9400 Loss5.7387\n",
      "Epoch 2 Batch 9500 Loss5.6561\n",
      "Epoch 2 Batch 9600 Loss5.4341\n",
      "Epoch 2 Batch 9700 Loss6.3178\n",
      "Epoch 2 Batch 9800 Loss5.6710\n",
      "Epoch 2 Batch 9900 Loss5.9067\n",
      "Epoch 2 Batch 10000 Loss6.5020\n",
      "Epoch 2 Batch 10100 Loss5.9937\n",
      "Epoch 2 Batch 10200 Loss5.6283\n",
      "Epoch 2 Batch 10300 Loss5.4995\n",
      "Epoch 2 Batch 10400 Loss5.6413\n",
      "Epoch 2 Batch 10500 Loss5.7319\n",
      "Epoch 2 Batch 10600 Loss5.8363\n",
      "Epoch 2 Batch 10700 Loss5.5623\n",
      "Epoch 2 Batch 10800 Loss6.0613\n",
      "Epoch 2 Batch 10900 Loss6.0785\n",
      "Epoch 2 Batch 11000 Loss5.7101\n",
      "Epoch 2 Batch 11100 Loss5.9384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 11200 Loss5.6349\n",
      "Epoch 2 Batch 11300 Loss5.3836\n",
      "Epoch 2 Batch 11400 Loss5.1710\n",
      "Epoch 2 Batch 11500 Loss5.3312\n",
      "Epoch 2 Batch 11600 Loss5.8600\n",
      "Epoch 2 Batch 11700 Loss5.8938\n",
      "Epoch 2 Batch 11800 Loss6.0598\n",
      "Epoch 2 Batch 11900 Loss5.6992\n",
      "Epoch 2 Batch 12000 Loss5.6496\n",
      "Epoch 2 Batch 12100 Loss5.6777\n",
      "Epoch 2 Batch 12200 Loss5.5619\n",
      "Epoch 2 Batch 12300 Loss5.3040\n",
      "Epoch 2 Batch 12400 Loss5.8656\n",
      "Epoch 2 Batch 12500 Loss5.8522\n",
      "Epoch 2 Batch 12600 Loss5.8139\n",
      "Epoch 2 Batch 12700 Loss6.2198\n",
      "Epoch 2 Batch 12800 Loss6.2507\n",
      "Epoch 2 Batch 12900 Loss6.1778\n",
      "Epoch 2 Batch 13000 Loss5.8944\n",
      "Epoch 2 Batch 13100 Loss6.4996\n",
      "Epoch 2 Batch 13200 Loss5.9483\n",
      "Epoch 2 Batch 13300 Loss5.4229\n",
      "Epoch 2 Batch 13400 Loss5.9396\n",
      "Epoch 2 Batch 13500 Loss5.0432\n",
      "Epoch 2 Batch 13600 Loss5.8320\n",
      "Epoch 2 Batch 13700 Loss5.7045\n",
      "Epoch 2 Batch 13800 Loss5.7670\n",
      "Epoch 2 Batch 13900 Loss5.8386\n",
      "Epoch 2 Batch 14000 Loss5.4210\n",
      "Epoch 2 Batch 14100 Loss5.6278\n",
      "Epoch 2 Batch 14200 Loss5.5313\n",
      "Epoch 2 Batch 14300 Loss5.6588\n",
      "Epoch 2 Batch 14400 Loss6.4360\n",
      "Epoch 2 Batch 14500 Loss5.4756\n",
      "Epoch 2 Batch 14600 Loss5.7791\n",
      "Epoch 2 Batch 14700 Loss6.0467\n",
      "Epoch 2 Batch 14800 Loss5.6693\n",
      "Epoch 2 Batch 14900 Loss5.9841\n",
      "Epoch 2 Batch 15000 Loss6.1131\n",
      "Epoch 2 Batch 15100 Loss5.8495\n",
      "Epoch 2 Batch 15200 Loss5.0609\n",
      "Epoch 2 Batch 15300 Loss6.0472\n",
      "Epoch 2 Batch 15400 Loss5.5345\n",
      "Epoch 2 Batch 15500 Loss5.3970\n",
      "Epoch 2 Batch 15600 Loss5.5136\n",
      "Epoch 2 Batch 15700 Loss5.1122\n",
      "Epoch 2 Batch 15800 Loss6.1453\n",
      "Epoch 2 Batch 15900 Loss5.0880\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    hidden = modelo.reset_states()\n",
    "    for (batch, (input, target)) in enumerate(ds):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, hidden = modelo(input, hidden)\n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    "            grads = tape.gradient(loss, modelo.variables)\n",
    "            optimizer.apply_gradients(zip(grads, modelo.variables))\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss{:.4f}'.format(epoch + 1, batch, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
